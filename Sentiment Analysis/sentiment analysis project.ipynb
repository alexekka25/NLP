{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0681cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, LSTM, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f5129",
   "metadata": {},
   "source": [
    "# Loading and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48cee512",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"D:\\Projects\\Sentiment_analysis\\movie.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0e5da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>\"Western Union\" is something of a forgotten cl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>This movie is an incredible piece of work. It ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>My wife and I watched this movie because we pl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>When I first watched Flatliners, I was amazed....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>Why would this film be so good, but only gross...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      I grew up (b. 1965) watching and loving the Th...      0\n",
       "1      When I put this movie in my DVD player, and sa...      0\n",
       "2      Why do people who do not know what a particula...      0\n",
       "3      Even though I have great interest in Biblical ...      0\n",
       "4      Im a die hard Dads Army fan and nothing will e...      1\n",
       "...                                                  ...    ...\n",
       "39995  \"Western Union\" is something of a forgotten cl...      1\n",
       "39996  This movie is an incredible piece of work. It ...      1\n",
       "39997  My wife and I watched this movie because we pl...      0\n",
       "39998  When I first watched Flatliners, I was amazed....      1\n",
       "39999  Why would this film be so good, but only gross...      1\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8db6e588",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cff227",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_length = 100  \n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa80cf",
   "metadata": {},
   "source": [
    "# Load Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037df7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "with open(r\"D:\\Projects\\Sentiment_analysis\\glove.6B\\glove.6B.100d.txt\", 'r', encoding='utf-8') as file:  # Use appropriate file path\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "embedding_dim = 100  # Dimension of embeddings\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4372a48b",
   "metadata": {},
   "source": [
    "# Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cf42ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_type='LSTM'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix],\n",
    "                        input_length=max_length, trainable=False))\n",
    "    if model_type == 'LSTM':\n",
    "        model.add(LSTM(128, return_sequences=False))\n",
    "    elif model_type == 'GRU':\n",
    "        model.add(GRU(128, return_sequences=False))\n",
    "    elif model_type == 'Bidirectional':\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff36962",
   "metadata": {},
   "source": [
    "# Training and Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1ec8140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALEX\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 122ms/step - accuracy: 0.5406 - loss: 0.6915 - val_accuracy: 0.5605 - val_loss: 0.6867\n",
      "Epoch 2/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 121ms/step - accuracy: 0.6241 - loss: 0.6369 - val_accuracy: 0.7742 - val_loss: 0.4743\n",
      "Epoch 3/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 121ms/step - accuracy: 0.7814 - loss: 0.4660 - val_accuracy: 0.7884 - val_loss: 0.4405\n",
      "Epoch 4/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 121ms/step - accuracy: 0.7956 - loss: 0.4373 - val_accuracy: 0.7905 - val_loss: 0.4306\n",
      "Epoch 5/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 121ms/step - accuracy: 0.8062 - loss: 0.4223 - val_accuracy: 0.8053 - val_loss: 0.4212\n",
      "Epoch 6/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 121ms/step - accuracy: 0.8111 - loss: 0.4102 - val_accuracy: 0.8008 - val_loss: 0.4206\n",
      "Epoch 7/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 122ms/step - accuracy: 0.8216 - loss: 0.3882 - val_accuracy: 0.8133 - val_loss: 0.4051\n",
      "Epoch 8/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 122ms/step - accuracy: 0.8278 - loss: 0.3783 - val_accuracy: 0.8128 - val_loss: 0.3999\n",
      "Epoch 9/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 121ms/step - accuracy: 0.8457 - loss: 0.3536 - val_accuracy: 0.8073 - val_loss: 0.4074\n",
      "Epoch 10/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 125ms/step - accuracy: 0.8518 - loss: 0.3393 - val_accuracy: 0.8206 - val_loss: 0.4216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GRU model...\n",
      "Epoch 1/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 119ms/step - accuracy: 0.5843 - loss: 0.6601 - val_accuracy: 0.7683 - val_loss: 0.4903\n",
      "Epoch 2/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 117ms/step - accuracy: 0.7869 - loss: 0.4625 - val_accuracy: 0.8025 - val_loss: 0.4242\n",
      "Epoch 3/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 118ms/step - accuracy: 0.8054 - loss: 0.4178 - val_accuracy: 0.8105 - val_loss: 0.4070\n",
      "Epoch 4/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 118ms/step - accuracy: 0.8209 - loss: 0.3918 - val_accuracy: 0.8152 - val_loss: 0.4027\n",
      "Epoch 5/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 118ms/step - accuracy: 0.8323 - loss: 0.3707 - val_accuracy: 0.8241 - val_loss: 0.3852\n",
      "Epoch 6/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 118ms/step - accuracy: 0.8418 - loss: 0.3494 - val_accuracy: 0.8150 - val_loss: 0.4283\n",
      "Epoch 7/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 117ms/step - accuracy: 0.8568 - loss: 0.3280 - val_accuracy: 0.8275 - val_loss: 0.3893\n",
      "Epoch 8/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 117ms/step - accuracy: 0.8703 - loss: 0.3012 - val_accuracy: 0.8255 - val_loss: 0.3835\n",
      "Epoch 9/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 115ms/step - accuracy: 0.8844 - loss: 0.2784 - val_accuracy: 0.8272 - val_loss: 0.4193\n",
      "Epoch 10/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 115ms/step - accuracy: 0.8944 - loss: 0.2571 - val_accuracy: 0.8294 - val_loss: 0.3887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bidirectional model...\n",
      "Epoch 1/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 211ms/step - accuracy: 0.6197 - loss: 0.6457 - val_accuracy: 0.7423 - val_loss: 0.5168\n",
      "Epoch 2/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 186ms/step - accuracy: 0.7559 - loss: 0.5068 - val_accuracy: 0.7497 - val_loss: 0.5012\n",
      "Epoch 3/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 185ms/step - accuracy: 0.7888 - loss: 0.4501 - val_accuracy: 0.8005 - val_loss: 0.4228\n",
      "Epoch 4/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 194ms/step - accuracy: 0.8109 - loss: 0.4115 - val_accuracy: 0.8022 - val_loss: 0.4213\n",
      "Epoch 5/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 187ms/step - accuracy: 0.8224 - loss: 0.3906 - val_accuracy: 0.8169 - val_loss: 0.3988\n",
      "Epoch 6/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 193ms/step - accuracy: 0.8364 - loss: 0.3622 - val_accuracy: 0.8188 - val_loss: 0.3918\n",
      "Epoch 7/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 215ms/step - accuracy: 0.8473 - loss: 0.3454 - val_accuracy: 0.8177 - val_loss: 0.4181\n",
      "Epoch 8/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 217ms/step - accuracy: 0.8587 - loss: 0.3225 - val_accuracy: 0.8163 - val_loss: 0.4331\n",
      "Epoch 9/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 216ms/step - accuracy: 0.8697 - loss: 0.2989 - val_accuracy: 0.8141 - val_loss: 0.4045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "models = ['LSTM', 'GRU', 'Bidirectional']\n",
    "saved_models = {}\n",
    "\n",
    "for model_type in models:\n",
    "    print(f\"Training {model_type} model...\")\n",
    "    model = build_model(model_type)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    model.fit(X_train_padded, y_train, validation_split=0.2, epochs=10, batch_size=64, callbacks=[early_stopping])\n",
    "    model.save(f'{model_type}_model.h5')  # Save model\n",
    "    saved_models[model_type] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77689c66",
   "metadata": {},
   "source": [
    "# Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7117e394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 34ms/step - accuracy: 0.8174 - loss: 0.3903\n",
      "LSTM Model - Loss: 0.3956148326396942, Accuracy: 0.8176249861717224\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.8279 - loss: 0.3792\n",
      "GRU Model - Loss: 0.384158730506897, Accuracy: 0.8255000114440918\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - accuracy: 0.8156 - loss: 0.4015\n",
      "Bidirectional Model - Loss: 0.39977532625198364, Accuracy: 0.8153749704360962\n"
     ]
    }
   ],
   "source": [
    "for model_type, model in saved_models.items():\n",
    "    loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "    print(f\"{model_type} Model - Loss: {loss}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17edec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import joblib\n",
    "\n",
    "\n",
    "joblib.dump(tokenizer, \"tokenizer.pkl\")\n",
    "print(\"Tokenizer saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55a271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
